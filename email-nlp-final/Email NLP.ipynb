{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email NLP\n",
    "#### Final project data science course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 \n",
    "previous part of the 3_4 part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 2\n",
      "num of features is 2294\n",
      "score is: 1.0\n",
      "[[25  0  0  0  0]\n",
      " [ 0 32  0  0  0]\n",
      " [ 0  0 95  0  0]\n",
      " [ 0  0  0 52  0]\n",
      " [ 0  0  0  0 27]]\n",
      "LR: 0.941486 (0.029299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Ben\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.804971 (0.059945)\n",
      "KNN: 0.813406 (0.047450)\n",
      "CART: 0.902565 (0.043191)\n",
      "NB: 0.874828 (0.042343)\n",
      "SVM: 0.554759 (0.071386)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGR9JREFUeJzt3X+cXXV95/HXuwFC+SHMbCJIEgir0ZJWiPU2tv7EattA\nqRS1mmiXH1XS9CHRhboFKVtiuyJ0F1AxOI02orUQsAqNu6nQXauglTYTNyDhlzGImQBlQkYChAQC\nn/5xzqSHy8y9587cn995Px+PeTzmnPO953y+c5P3Pfd7fikiMDOztPxcpwswM7Pmc7ibmSXI4W5m\nliCHu5lZghzuZmYJcribmSXI4W5jknSNpP/RonW/X9ItNZafKGmoFdvudZIulPSFTtdh3c/hPsVJ\n+rakEUnT27XNiPjbiPjNQg0h6RXt2r4yH5Z0l6SnJA1J+qqkV7erhomKiEsi4oOdrsO6n8N9CpM0\nF3gTEMA72rTN/dqxnTo+DXwE+DDQD7wSuAn47U4WVU+X/O2sRzjcp7bTgduBa4AzajWU9CeSHpb0\nkKQPFve2JR0m6cuShiU9KOkiST+XLztT0vckXSnpMWBFPu+7+fJb803cIelJSe8tbPOPJT2ab/es\nwvxrJF0t6R/y13xP0pGSPpV/C7lX0mvG6cc84EPAkoj4VkTsiYhd+beJSxvsz88kbZH0+nz+1rze\nM6pqHZD0j5KekPQdSccUln86f91OSRskvamwbIWkv5P0FUk7gTPzeV/Jlx+YL3ssr2W9pCPyZUdJ\nWitph6TNks6uWu8NeR+fkLRJUqXW+2+9x+E+tZ0O/G3+81ujwVBN0iLgPODtwCuAE6uaXAUcBvxn\n4C35es8qLH8dsAU4AvhE8YUR8eb81xMi4pCIuD6fPjJf5yzgA8BKSX2Fl74HuAiYAewBvg/8IJ/+\nO+CKcfr8NmAoIv51nOVl+3Mn8J+Aa4E1wK+Q/W1+H/ispEMK7d8P/EVe20ayv/eo9cACsm8Q1wJf\nlXRgYfmpeX8Or3odZB/IhwFz8lqWAU/ny9YAQ8BRwLuBSyT9euG178jbHA6sBT5b4+9hPcjhPkVJ\neiNwDHBDRGwAfgy8b5zm7wG+GBGbImIXsKKwnmnAYuBjEfFERPwEuBz4L4XXPxQRV0XE3oh4mnKe\nBf48Ip6NiHXAk8CrCstvjIgNEbEbuBHYHRFfjojngOuBMffcyULw4fE2WrI/D0TEFwvbmpPXuici\nbgGeIQv6Uf8nIm6NiD3AnwK/JmkOQER8JSIey/82lwPTq/r5/Yi4KSKeH+Nv92zen1dExHP532Nn\nvu43AOdHxO6I2Ah8gexDatR3I2Jd3oe/AU4Y729ivcnhPnWdAdwSEdvz6WsZf2jmKGBrYbr4+wxg\nf+DBwrwHyfa4x2pf1mMRsbcwvQso7g3/W+H3p8eYLrZ9wXqBl9XYbpn+VG+LiKi1/X39j4gngR1k\nf1MkfVTSPZIel/Qzsj3xGWO9dgx/A9wMrMmHy/5S0v75undExBM1+vBI4fddwIEe00+Lw30KkvTz\nZHvjb5H0iKRHgHOBEySNtQf3MDC7MD2n8Pt2sj3IYwrzjga2Faa76daj/w+YXWOMuUx/GrXv75UP\n1/QDD+Xj639C9l70RcThwOOACq8d92+Xf6v5eETMB14PnEK2d/4Q0C/p0Cb2wXqMw31q+l3gOWA+\n2XjvAuA44DZe+NV91A3AWZKOk3QQ8N9HF+Rf628APiHp0Pxg4XnAVxqo59/IxrdbLiJ+BFwNXKfs\nfPoD8gOTiyVd0KT+VDtZ0hslHUA29n57RGwFDgX2AsPAfpL+DHhJ2ZVKequkV+dDSTvJPpSez9f9\nz8An874dT3bcYjJ9sB7jcJ+aziAbQ/9pRDwy+kN2UO391V/PI+IfgM8A/wRsJjvDBrIDmQDLgafI\nDpp+l2yIZ3UD9awAvpSf8fGeCfapER8m6+tK4GdkxxtOA76RL59sf6pdC1xMNhzzWrKDrpANqXwT\nuJ9s2GQ3jQ1hHUl2sHUncA/wHbKhGoAlwFyyvfgbgYsj4v9Oog/WY+SHdVijJB0H3AVMrxoXtyqS\nriE7O+eiTtdiU4v33K0USadJmp6fjngZ8A0Hu1n3crhbWX8IPEo2hPEc8EedLcfMavGwjJlZgrzn\nbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZgly\nuJuZJcjhbmaWIIe7mVmCOva08xkzZsTcuXM7tXkzs560YcOG7RExs167uuEuaTXZU9UfjYhfGmO5\ngE8DJwO7gDMj4gf11jt37lwGBwfrNTMzswJJD5ZpV2ZY5hpgUY3lJwHz8p+lwOfKbNjMzFqnbrhH\nxK1kT20fz6nAlyNzO3C4pJc1q0AzM2tcMw6ozgK2FqaH8nlmZtYhbT1bRtJSSYOSBoeHh9u5aTOz\nKaUZ4b4NmFOYnp3Pe5GIWBURlYiozJxZ92CvmZlNUDPCfS1wujK/CjweEQ83Yb1mZjZBZU6FvA44\nEZghaQi4GNgfICIGgHVkp0FuJjsV8qxWFWtmZuXUDfeIWFJneQAfalpFZmY2aR27QrUVsuupJib7\njDIzS0NS4V4roCU5wM1syvCNw8zMEuRwNzNLkMPdzCxBDnczswT13AHV/v5+RkZGJvTaiZxN09fX\nx44dte6bZlbORM/m8okANhE9F+4jIyNt/cc+mdMrzYp8Npe1k4dlzMwS5HA3M0uQw93MLEEOdzOz\nBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS1HNXqJpZ9/GDcrqPw93MJs23Vug+HpYxa6L+/n4k\nNfwDNPya/v7+DvfWulnP7bnHxS+BFYe1d3tdbPny5Xz+859nz549TJ8+nbPPPpurrrqq02VNWe28\nsZ1vame19Fy46+M7235XyFjRts01ZPny5QwMDHDZZZexbNkyBgYGOP/88wEc8GZTnDo1FlapVGJw\ncLDh17V7/K6bxwsPPPBALrnkEs4777x986644gouvPBCdu/e3cHKpq52/nvp5n+bRb1SZ6+QtCEi\nKnXbOdy7a3uNkMRTTz3FQQcdtG/erl27OPjgg7u25tQ53F+sV+rsFWXDvScPqE7kgNVEf/r6+jrd\n3XFNnz6dgYGBF8wbGBhg+vTpHarIzLpFz425T3QPIMW9h7PPPnvfGHtxzH3ZsmUdrsxS5Edc9pae\nG5aZqBTDHXy2TNdp45lc2fYeb9umPCTaHZo65i5pEfBpYBrwhYi4tGp5H7AaeDmwG/iDiLir1jod\n7pailMfcU99er2jamLukacBK4CRgPrBE0vyqZhcCGyPieOB0sg8CMzPrkDIHVBcCmyNiS0Q8A6wB\nTq1qMx/4FkBE3AvMlXREUystYTJXAJqZpaRMuM8Cthamh/J5RXcA7wSQtBA4BphdvSJJSyUNShoc\nHh6eWMU1RMSEf8zMUtKsUyEvBQ6XtBFYDvx/4LnqRhGxKiIqEVGZOXNmkzZtZmbVypwKuQ2YU5ie\nnc/bJyJ2AmcBKBvjeADY0qQazcysQWX23NcD8yQdK+kAYDGwtthA0uH5MoAPArfmgW9mZh1Qd889\nIvZKOge4mexUyNURsUnSsnz5AHAc8CVJAWwCPtDCms3MrI5SV6hGxDpgXdW8gcLv3wde2dzSzMxs\nonry3jJmZlZbz91bxqzbteu6iXbf1M4PyuktDnezJkr5xnZ+UE5v8bCMmVmCHO5mZgnysIx1jcmM\nVXf7kIZZuzncrWvUCuheGJM26yYeljEzS5DD3cwsQQ53M7MEOdzNzBLkcLe26u/vr/lErIk+SWu8\nn/7+/g732KwzfLaMtdXIyEjbr3I0m4q8525mliCHu5lZghzuZmYJ8pi7mZXWzmMY7b6lcWoc7mZW\nSsq3M06Rh2XMzBLkPXdrKz/Nx6w9HO7WVn6aj1l7eFjGzCxBDnczswQ53M3MEuRwNzNLkMPdzCxB\npcJd0iJJ90naLOmCMZYfJukbku6QtEnSWc0v1czMyqob7pKmASuBk4D5wBJJ86uafQi4OyJOAE4E\nLpd0QJNrNTOzksqc574Q2BwRWwAkrQFOBe4utAngUGU3njgE2AHsbXKtU95k7uvhy787r977N97y\nXnjvJto36I3+9aIywzKzgK2F6aF8XtFngeOAh4AfAh+JiOerVyRpqaRBSYPDw8MTLDlttZ5UNBl+\nUlHnRcSEfnrBRPvWK/3rRc06oPpbwEbgKGAB8FlJL7ruOyJWRUQlIiozZ85s0qbTMvqkonb9jIyM\ndLrLZtYCZcJ9GzCnMD07n1d0FvD1yGwGHgB+oTklmplZo8qE+3pgnqRj84Oki4G1VW1+CrwNQNIR\nwKuALc0s1MzMyqt7QDUi9ko6B7gZmAasjohNkpblyweAvwCukfRDQMD5EbG9hXUny3dNNLNmUKcO\naFQqlRgcHOzItrtZux9s4O2Z9RZJGyKiUq+dr1A1M0uQw93MLEEOdzOzBDnczcwS5MfsdaHJXo3a\niL6+vrZty8zax+HeZSZ6ZkcvnRXiDy+z1nO4W1tNhQ8vs27gMXczswQ53M3MEuRwNzNLkMPdzCxB\nDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuTbD/SQevdkqbW8Fy7dT71/Zu3kcO8hqQdY\n6v0zaycPy5iZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZgkqFe6SFkm6T9Jm\nSReMsfy/SdqY/9wl6TlJ/c0v18zMyqgb7pKmASuBk4D5wBJJ84ttIuJ/RsSCiFgAfAz4TkTsaEXB\nZmZWX5k994XA5ojYEhHPAGuAU2u0XwJc14zizMxsYsqE+yxga2F6KJ/3IpIOAhYBXxtn+VJJg5IG\nh4eHG63VzMxKavYB1d8BvjfekExErIqISkRUZs6c2eRNm5nZqDLhvg2YU5ienc8by2I8JGNm1nFl\nwn09ME/SsZIOIAvwtdWNJB0GvAX4++aWaGZmjap7P/eI2CvpHOBmYBqwOiI2SVqWLx/Im54G3BIR\nT7WsWjMzK0WdekBCpVKJwcHBjmzbzKxXSdoQEZV67XyFqplZghzuZmYJ8jNUzczqqPfw9lo6NfTt\ncDczq6NWQEvqyoe7e1jGzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPd\nzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRw\nNzNLkMPdzCxBDnczswSVCndJiyTdJ2mzpAvGaXOipI2SNkn6TnPLNDOzRuxXr4GkacBK4DeAIWC9\npLURcXehzeHA1cCiiPippJe2qmAzM6uvzJ77QmBzRGyJiGeANcCpVW3eB3w9In4KEBGPNrdMMzNr\nRJlwnwVsLUwP5fOKXgn0Sfq2pA2STh9rRZKWShqUNDg8PDyxis3MWqC/vx9JDf8AE3pdf39/S/tT\nd1imgfW8Fngb8PPA9yXdHhH3FxtFxCpgFUClUokmbdvMbNJGRkaIaF8sjX4wtEqZcN8GzClMz87n\nFQ0Bj0XEU8BTkm4FTgDux8zM2q7MsMx6YJ6kYyUdACwG1la1+XvgjZL2k3QQ8DrgnuaWamZmZdXd\nc4+IvZLOAW4GpgGrI2KTpGX58oGIuEfSN4E7geeBL0TEXa0s3MzMxqd2jjEVVSqVGBwc7Mi2zcyq\nSWr7mPtEtidpQ0RU6rXzFapmZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmC\nmnXjMDOznhYXvwRWHNbe7bWQw93MDNDHd7b/CtUVrVu/h2XMzBLkcDczS5DD3cwsQQ53M7MEOdzN\nzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53\nM7MElQp3SYsk3Sdps6QLxlh+oqTHJW3Mf/6s+aWamVlZdZ/EJGkasBL4DWAIWC9pbUTcXdX0tog4\npQU1mplZg8o8Zm8hsDkitgBIWgOcClSHu5lZT5PUtm319fW1dP1lwn0WsLUwPQS8box2r5d0J7AN\n+GhEbGpCfWZmbTHR56dKauuzV8tq1gOyfwAcHRFPSjoZuAmYV91I0lJgKcDRRx/dpE2bmVm1MgdU\ntwFzCtOz83n7RMTOiHgy/30dsL+kGdUriohVEVGJiMrMmTMnUbaZmdVSJtzXA/MkHSvpAGAxsLbY\nQNKRygerJC3M1/tYs4s1M7Ny6g7LRMReSecANwPTgNURsUnSsnz5APBu4I8k7QWeBhZHNw5CmZlN\nEepUBlcqlRgcHOzIts3MmqXdB1QlbYiISr12vkLVzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDncz\nswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPd\nzCxBDnczswQ53M3MEuRwNzNLkMPdzCxB+3W6ADOzbidpwssjotnllOJwNzOro1MBPRkeljEzS5DD\n3cwsQQ53M7MEOdzNzBJUKtwlLZJ0n6TNki6o0e5XJO2V9O7mlWhmZo2qG+6SpgErgZOA+cASSfPH\naXcZcEuzizQzs8aU2XNfCGyOiC0R8QywBjh1jHbLga8BjzaxPjMzm4Ay4T4L2FqYHsrn7SNpFnAa\n8LlaK5K0VNKgpMHh4eFGazUzs5KadRHTp4DzI+L5OldqrQJWAUgalvRgk7Zfxgxgexu3127uX29L\nuX8p9w3a379jyjQqE+7bgDmF6dn5vKIKsCYP9hnAyZL2RsRN4600ImaWKbBZJA1GRKWd22wn96+3\npdy/lPsG3du/MuG+Hpgn6ViyUF8MvK/YICKOHf1d0jXA/64V7GZm1lp1wz0i9ko6B7gZmAasjohN\nkpblywdaXKOZmTWo1Jh7RKwD1lXNGzPUI+LMyZfVEqs6XUCLuX+9LeX+pdw36NL+qRfvdmZmZrX5\n9gNmZglKMtwlPTnGvBWStknaKOluSUs6UdtElOjPjyR9vfrKYUkzJD07enykGxX7JulkSfdLOibv\n3y5JLx2nbUi6vDD9UUkr2lZ4HZKOlLRG0o8lbZC0TtIr82X/VdJuSYcV2p8o6fH8/bxX0v/K55+V\nz9so6RlJP8x/v7RTfRtPrfek6t/rvZI+J6nr80fSn0raJOnOvPaLJX2yqs0CSffkv/9E0m1VyzdK\nuquddUOi4V7DlRGxgOwK27+StH+nC5qkKyNiQUTMA64HviWpeIrp7wG3A13/QSbpbcBngJMiYvT6\nh+3AH4/zkj3AOyXNaEd9jVB2TvCNwLcj4uUR8VrgY8AReZMlZGehvbPqpbfl/z5fA5wi6Q0R8cX8\nPV4APAS8NZ8e9x5PHVTvPRn9/zcfeDXwlrZVNgGSfg04BfjliDgeeDvwT8B7q5ouBq4rTB8qaU6+\njuPaUetYplq4AxARPwJ2AX2drqVZIuJ6svv6FE9TXUIWjrMkze5IYSVIejPweeCUiPhxYdFq4L2S\n+sd42V6yA1nntqHERr0VeLZ40kFE3BERt0l6OXAIcBHjfOhGxNPARqquBO8BZd+TA4ADgZGWVzQ5\nLwO2R8QegIjYHhG3AiOSXldo9x5eGO438B8fAEuqlrXNlAx3Sb8M/CgiUrsPzg+AXwDI9xxeFhH/\nygv/sXWb6cBNwO9GxL1Vy54kC/iPjPPalcD7i8MbXeKXgA3jLFtMdn+m24BXSTqiuoGkPmAecGvL\nKmydWu/JuZI2Ag8D90fExvaW1rBbgDn5UOHVkka/aVxH9j4i6VeBHfkO46iv8R/fyn4H+Ea7Ci6a\nauF+rqRNwL8An+h0MS1QvPfDe8lCHbIw6dahmWeBfwY+MM7yzwBnSDq0ekFE7AS+DHy4deU13RJg\nTUQ8TxYCv1dY9iZJd5BdLHhzRDzSiQIno857Mjos81LgYEmL21pcgyLiSeC1wFJgGLhe0plkQ6Dv\nzo8ZVA/JADxGtne/GLiHbJSg7aZauF8ZEb8IvAv4a0kHdrqgJnsN2T8myELkTEk/AdYCx0ua16nC\nanie7GvtQkkXVi+MiJ8B1wIfGuf1nyL7YDi4ZRU2bhNZKLyApFeT7ZH/Y/6+LOaFH7q3RcQJwC8C\nH5C0oA21tkLN9yQingW+Cby5nUVNREQ8FxHfjoiLgXOAd0XEVuABsmMG7yIL+2rXk32L6ciQDEy9\ncAcgItYCg8AZna6lWSS9C/hN4Lr8rIxDImJWRMyNiLnAJ+nSvfeI2AX8NtnX+bH24K8A/pAxLrqL\niB1k31DG2/PvhG8B0yUtHZ0h6XiybyErRt+TiDgKOErSC24EFREPAJcC57ez6Gap957kB5zfAPx4\nrOXdQtKrqnaIFgCjB/uvA64EtkTE0BgvvxH4S7Ir+zsi1XA/SNJQ4ee8Mdr8OXBeL5yOxfj9OXf0\nVEjg94Ffj4hhshC/sWodX6NLwx32BcIi4CJJ76hatp2sP9PHefnlZDes6wqRXRl4GvD2/FTITWQf\nrify4vflRvLx2yoDwJslzW1dpS011nsyOuZ+F9mtTK5ue1WNOQT4krJTp+8kO8tnRb7sq2TfsMbc\nM4+IJyLisvwZGB3hK1TNzBLUC3utZmbWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZgly\nuJuZJejfAdcGwbgibU5LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125767b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import gensim\n",
    "import re\n",
    "from gensim import corpora, models\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def buildAndCleanCSV(fileName,sendersArray):\n",
    "    dictionariesOfSenders={}\n",
    "    allWordsOfSenders={}\n",
    "    row_count=0\n",
    "    for sender in sendersArray:\n",
    "        dictionariesOfSenders[sender]=[]\n",
    "\n",
    "    with open(fileName+'.csv', 'rb') as csv_file:\n",
    "        csv.field_size_limit(sys.maxint)\n",
    "        reader = csv.reader(csv_file)\n",
    "        row_count = sum(1 for row in reader)\n",
    "    with open(fileName+'.csv', 'rb') as csv_file:\n",
    "        csv.field_size_limit(sys.maxint)\n",
    "        reader = csv.reader(csv_file)\n",
    "        current_row=1\n",
    "        line = next(reader, None)\n",
    "        while (line):\n",
    "            if current_row % 1000==0:\n",
    "                print('row is: '+str(current_row)+'/'+str(row_count))\n",
    "            # Remove HTML\n",
    "            text = BeautifulSoup(line[1], \"html.parser\").get_text()\n",
    "            #  Remove non-letters and words longer than 3 letters\n",
    "            letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "            letters_only = ' '.join(word for word in letters_only.split() if len(word) > 2)\n",
    "            #lower letters\n",
    "            letters_only = letters_only.lower()\n",
    "            # Convert to lower case, split into individual words\n",
    "            words = letters_only.split()\n",
    "            # a list, so convert the stop words to a set (faster)\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            emailStopWords = {'Additional', 'option' ,'for', 'viewing',' and', 'saving', 'the', 'attached', 'documents','http','mass','bgu','ac' ,'il', 'nm', 'php' ,'mm' ,'b' ,'cfe' ,'a','Website', 'www', 'bgu','attachments', 'view', 'npdf', 'pdf', 'alternative', 'view','save', 'additional'}\n",
    "            # Remove stop words\n",
    "            meaningful_words = [w for w in words if not w in stops]\n",
    "            meaningful_words = [w for w in meaningful_words if not w in emailStopWords]\n",
    "            # 6. Join the words back into one string separated by space and return the result.\n",
    "            words_combined=\" \".join(meaningful_words)\n",
    "            #stemming\n",
    "            #porter = nltk.PorterStemmer()\n",
    "            #[porter.stem(w) for w in words_combined]\n",
    "            #taking only emails with more than 50 characters to avoid non-informative emails\n",
    "            if(len(words_combined)>30):\n",
    "                dictionariesOfSenders[line[0]].append(words_combined.split())\n",
    "            line = next(reader, None)\n",
    "            current_row+=1\n",
    "    sendersBOWlength=[]\n",
    "    for sender in sendersArray:\n",
    "        combined=[]\n",
    "        for tmp in dictionariesOfSenders[sender]:\n",
    "            combined=combined+tmp\n",
    "        allWordsOfSenders[sender]=set(combined)\n",
    "        #BOW size will be the number of different words and if its above 5000, max size will be set to 5000\n",
    "        sendersBOWlength.append(len(allWordsOfSenders[sender]) if len(allWordsOfSenders[sender])<=5000 else 5000)\n",
    "    #creating the bag of words\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "    sendersVectorizers_featureNames={}\n",
    "    sendersVectorizers_arrays={}\n",
    "    sendersVectorizers_models={}\n",
    "    #combine all clean emails from all\n",
    "    combined_y_label=[]\n",
    "    text_representation = []\n",
    "    for sender in sendersArray:\n",
    "        for emailWordsList in dictionariesOfSenders[sender]:\n",
    "            combined_y_label.append(sender)\n",
    "            text_representation.append(\" \".join(emailWordsList))\n",
    "    #vectorizing, we chose max_features to be the max size of the BOW texts length or if higher than 5000 the size will be set to 5000\n",
    "    num_of_features=max(sendersBOWlength)\n",
    "    print('num of features is '+str(num_of_features))\n",
    "    sendersVectorizers= CountVectorizer(analyzer=\"word\", \\\n",
    "                             tokenizer=None, \\\n",
    "                             preprocessor=None, \\\n",
    "                             stop_words=None, \\\n",
    "                             max_features=num_of_features)\n",
    "    #reading the new prediction test part to have the same number of features\n",
    "    (pred_data_test_x,pred_data_test_y)=readPredData(senderlist)\n",
    "    #merging test and train for the vectorization\n",
    "    merged_list=text_representation+pred_data_test_x\n",
    "    #sendersVectorizers_model = sendersVectorizers.fit_transform(text_representation)\n",
    "    sendersVectorizers_model = sendersVectorizers.fit_transform(merged_list)\n",
    "    sendersVectorizers_arrays=sendersVectorizers_model.toarray()\n",
    "    # seperate train and test\n",
    "    train_index=len(text_representation)\n",
    "    #test needs to be 30% of the size of the train\n",
    "    test_index=int(train_index*0.3)\n",
    "    splitted_train_data=sendersVectorizers_arrays[:train_index]\n",
    "    splitted_test_data=sendersVectorizers_arrays[train_index:]\n",
    "    #creating the random forest classifier\n",
    "    train_data_for_all_x=splitted_train_data\n",
    "    train_data_for_all_y=combined_y_label\n",
    "    #shuffle the test arrays and choose only 30%\n",
    "    c2 = list(zip(splitted_test_data, pred_data_test_y))\n",
    "    random.shuffle(c2)\n",
    "    splitted_test_data, pred_data_test_y = zip(*c2)\n",
    "    splitted_test_data=splitted_test_data[:test_index]\n",
    "    pred_data_test_y=pred_data_test_y[:test_index]\n",
    "    #shuffle the arrays\n",
    "    c = list(zip(train_data_for_all_x, train_data_for_all_y))\n",
    "    random.shuffle(c)\n",
    "    train_data_for_all_x, train_data_for_all_y = zip(*c)\n",
    "    # split to train & test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data_for_all_x, train_data_for_all_y, test_size=0.2)\n",
    "\n",
    "    # Initialize a Random Forest classifier with 100 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 100)\n",
    "    # Fit the forest to the training set, using the bag of words as\n",
    "    # features and the sentiment labels as the response variable\n",
    "    #\n",
    "    # This may take a few minutes to run\n",
    "    forest = forest.fit( x_train, y_train )\n",
    "    #joblib.dump(forest, 'model.pkl', compress=9)\n",
    "\n",
    "    y_test_pred=forest.predict(splitted_test_data)\n",
    "    # Evaluate accuracy best on the test set\n",
    "    print('score is: ' +str(forest.score(splitted_test_data,pred_data_test_y)))\n",
    "    #the confusion matrix of the senders of the test data (for random forest):\n",
    "    print(str(confusion_matrix(pred_data_test_y, y_test_pred)))\n",
    "    #comparing more models:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.svm import SVC\n",
    "    # prepare configuration for cross validation test harness\n",
    "    seed = 7\n",
    "    # prepare models\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = 'accuracy'\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()\n",
    "\n",
    "    print('done')\n",
    "def readPredData(senderlist):\n",
    "    dictionariesOfSenders = {}\n",
    "    text_representation_senders = []\n",
    "    y_labels_representation_senders=[]\n",
    "    row_count = 0\n",
    "    for sender in senderlist:\n",
    "        dictionariesOfSenders[sender] = []\n",
    "        with open(sender + '_pred.csv', 'rb') as csv_file:\n",
    "            csv.field_size_limit(sys.maxint)\n",
    "            reader = csv.reader(csv_file)\n",
    "            line = next(reader, None)\n",
    "            while (line):\n",
    "                pred_email = line[0]\n",
    "                # Remove HTML\n",
    "                text = BeautifulSoup(pred_email, \"html.parser\").get_text()\n",
    "                #  Remove non-letters and words longer than 3 letters\n",
    "                letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "                letters_only = ' '.join(word for word in letters_only.split() if len(word) > 2)\n",
    "                # lower letters\n",
    "                letters_only = letters_only.lower()\n",
    "                # Convert to lower case, split into individual words\n",
    "                words = letters_only.split()\n",
    "                # a list, so convert the stop words to a set (faster)\n",
    "                stops = set(stopwords.words(\"english\"))\n",
    "                emailStopWords = {'Additional', 'option', 'for', 'viewing', ' and', 'saving', 'the', 'attached',\n",
    "                                  'documents', 'http', 'mass', 'bgu', 'ac', 'il', 'nm', 'php', 'mm', 'b', 'cfe', 'a',\n",
    "                                  'Website', 'www', 'bgu', 'attachments', 'view', 'npdf', 'pdf', 'alternative', 'view',\n",
    "                                  'save', 'additional'}\n",
    "                # Remove stop words\n",
    "                meaningful_words = [w for w in words if not w in stops]\n",
    "                meaningful_words = [w for w in meaningful_words if not w in emailStopWords]\n",
    "                # 6. Join the words back into one string separated by space and return the result.\n",
    "                words_combined = \" \".join(meaningful_words)\n",
    "                dictionariesOfSenders[sender].append(words_combined.split())\n",
    "                line = next(reader, None)\n",
    "\n",
    "            combined_y_label = []\n",
    "            text_representation = []\n",
    "            for emailWordsList in dictionariesOfSenders[sender]:\n",
    "                combined_y_label.append(sender)\n",
    "                text_representation.append(\" \".join(emailWordsList))\n",
    "            text_representation_senders=text_representation_senders+text_representation\n",
    "            y_labels_representation_senders=y_labels_representation_senders+combined_y_label\n",
    "    return (text_representation_senders,y_labels_representation_senders)\n",
    "#main\n",
    "print('part 2')\n",
    "\n",
    "#nltk.download()\n",
    "senderlist = ['dean@bgu.ac.il', 'peler@exchange.bgu.ac.il', 'bitahon@bgu.ac.il', 'career@bgu.ac.il',\n",
    "              'shanigu@bgu.ac.il']\n",
    "#we decided to use the code from part two with the trained data\n",
    "#because we needed the features number to be the same as in the training data\n",
    "#so we used the new predicted data as the test data.\n",
    "#we also used this part methods because we wanted the data to be precessed with the same preprocessing\n",
    "buildAndCleanCSV('filteredBySendersTranslated',senderlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final part\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\Anaconda2\\lib\\site-packages\\keras\\preprocessing\\text.py:139: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is: (32669L, 800L) output shape is: (32669L, 800L)\n",
      "Loaded model from disk\n",
      "sender dean@bgu.ac.ilaccuracy is: \n",
      "acc: 41.08%\n",
      "input shape is: (15154L, 800L) output shape is: (15154L, 800L)\n",
      "Loaded model from disk\n",
      "sender peler@exchange.bgu.ac.ilaccuracy is: \n",
      "acc: 36.87%\n",
      "input shape is: (13010L, 800L) output shape is: (13010L, 800L)\n",
      "Loaded model from disk\n",
      "sender bitahon@bgu.ac.ilaccuracy is: \n",
      "acc: 52.85%\n",
      "input shape is: (21304L, 800L) output shape is: (21304L, 800L)\n",
      "Loaded model from disk\n",
      "sender career@bgu.ac.ilaccuracy is: \n",
      "acc: 41.03%\n",
      "input shape is: (11071L, 800L) output shape is: (11071L, 800L)\n",
      "Loaded model from disk\n",
      "sender shanigu@bgu.ac.ilaccuracy is: \n",
      "acc: 57.30%\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "import itertools\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import model_from_json\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "\n",
    "\n",
    "def createModels(fileName,senderlist):\n",
    "    #read and seperate each sender\n",
    "    with open(fileName+'.csv', 'rb') as csv_file:\n",
    "        csv.field_size_limit(sys.maxint)\n",
    "        reader = csv.reader(csv_file)\n",
    "        sendersWords = {}\n",
    "        for k in range(0, len(senderlist)):\n",
    "            sendersWords[senderlist[k]] = []\n",
    "        line = next(reader, None)\n",
    "        while (line):\n",
    "            sender = line[0]\n",
    "            splitEmail = line[1];\n",
    "            sendersWords[sender].append(splitEmail)\n",
    "            line = next(reader, None)\n",
    "        #starting working with keras to create the model for each sender\n",
    "        for k in range(0, len(senderlist)):\n",
    "            senderEmailsArr=sendersWords[senderlist[k]];\n",
    "            model = Sequential()\n",
    "            n_hidden = 256\n",
    "            n_fac = 42\n",
    "            input_list=[]\n",
    "            output_list=[]\n",
    "            #list for each sentence with first word and number of words\n",
    "            first_word_list=[]\n",
    "            allEmails=\"\"\n",
    "            for email in senderEmailsArr:\n",
    "                allEmails=allEmails+email+\"--\"\n",
    "                emailSplit=email.split(\" \")\n",
    "                firstWord=\"\"\n",
    "                for i in range(0,len(emailSplit)):\n",
    "                    if (emailSplit[i]!=''):\n",
    "                        firstWord=emailSplit[i]\n",
    "                        break\n",
    "                first_word_list.append((firstWord, len(emailSplit)))\n",
    "            allEmails=allEmails[:-2]\n",
    "            vocabulary_size = 800\n",
    "            unknown_token = \"UNKNOWNTOKEN\"\n",
    "            sentence_start_token = \"SENTENCESTART\"\n",
    "            sentence_end_token = \"SENTENCEEND\"\n",
    "            line_break = \"NEWLINE\"\n",
    "            separator = \"SEPARATOR\"\n",
    "            emailText = allEmails.replace('\\n', ' ' + line_break + ' ')\n",
    "            emailText = emailText.replace('--', ' ' + separator + ' ')\n",
    "            emailText = emailText.replace('.', ' ' + sentence_end_token + ' ' + sentence_start_token + ' ')\n",
    "            emailText = re.sub(r'\\d+', '', emailText)\n",
    "            emailTextSeq = text_to_word_sequence(emailText, lower=True, split=\" \")  # using only 10000 first words\n",
    "\n",
    "            token = Tokenizer(nb_words=vocabulary_size, char_level=False)\n",
    "            token.fit_on_texts(emailTextSeq)\n",
    "            text_mtx = token.texts_to_matrix(emailTextSeq, mode='binary')\n",
    "            input_ = text_mtx[:-1]\n",
    "            output_ = text_mtx[1:]\n",
    "            print('input shape is: '+str(input_.shape)+' output shape is: '+ str(output_.shape))\n",
    "            #training the model\n",
    "            model.add(Embedding(input_dim=input_.shape[1], output_dim=42, input_length=input_.shape[1]))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(output_.shape[1], activation='sigmoid'))\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
    "            model.fit(input_, y=output_, batch_size=500, nb_epoch=50, verbose=1, validation_split=0.2)\n",
    "            saveModelToFile(model, senderlist[k])\n",
    "            with open(senderlist[k] + '_pred.csv', 'wb') as result_csv_file:\n",
    "                writer = csv.writer(result_csv_file)\n",
    "                for pred_email in first_word_list:\n",
    "                    pred_text=\"\"\n",
    "                    nextWord=separator\n",
    "                    for i in range(0,pred_email[1]):\n",
    "                        if(i==vocabulary_size-1):\n",
    "                            break\n",
    "                        try:\n",
    "                            pred_word=get_next(nextWord, token, model, text_mtx, emailTextSeq)\n",
    "                            pred_text=pred_text+\" \"+pred_word\n",
    "                            nextWord=pred_word\n",
    "                        except ValueError:\n",
    "                            print nextWord\n",
    "                        if (nextWord == separator):\n",
    "                            break\n",
    "                    writer.writerow([pred_text])\n",
    "            # evaluate the model\n",
    "            #scores = model.evaluate(input_, output_, verbose=0)\n",
    "            #print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "            saveModelToFile(model,senderlist[k])\n",
    "\n",
    "\n",
    "def loadModelFromFile(name):\n",
    "    # load json and create model\n",
    "    json_file = open(name+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(name+\".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "\n",
    "def saveModelToFile(model,name):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(name+\".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "#a function to predict the next word\n",
    "def get_next(text,token,model,fullmtx,fullText):\n",
    "    tmp = text_to_word_sequence(text, lower=True, split=\" \")\n",
    "    tmp = token.texts_to_matrix(tmp, mode='binary')\n",
    "    p = model.predict(tmp)\n",
    "    bestMatch = np.min(np.argmax(p))\n",
    "    options=np.where(fullmtx[:,bestMatch]>0)[0]\n",
    "    smart_rand_pos=random.randint(0,len(options)-1)\n",
    "    next_idx = smart_rand_pos\n",
    "    return fullText[next_idx]\n",
    "\n",
    "def readClassifier(file_name):\n",
    "    return joblib.load(file_name)\n",
    "\n",
    "\n",
    "def read_random_parts(file_name, percentage):\n",
    "    lines = sum(1 for line in open(file_name)) #number of records in file (excludes header)\n",
    "    print lines\n",
    "    C = int(lines * (percentage/100.0)) #sample size (percentage%)\n",
    "    buffer = []\n",
    "    f = open(file_name, 'r')\n",
    "    for line_num, line in enumerate(f):\n",
    "        n = line_num + 1.0\n",
    "        r = random.random()\n",
    "        if n <= C:\n",
    "            buffer.append(line.strip())\n",
    "        elif r < C/n:\n",
    "            loc = random.randint(0, C-1)\n",
    "            buffer[loc] = line.strip()\n",
    "    return buffer\n",
    "\n",
    "def evaluateTheModel(fileName,senderlist):\n",
    "    # read and seperate each sender\n",
    "    with open(fileName + '.csv', 'rb') as csv_file:\n",
    "        csv.field_size_limit(sys.maxint)\n",
    "        reader = csv.reader(csv_file)\n",
    "        sendersWords = {}\n",
    "        for k in range(0, len(senderlist)):\n",
    "            sendersWords[senderlist[k]] = []\n",
    "        line = next(reader, None)\n",
    "        while (line):\n",
    "            sender = line[0]\n",
    "            splitEmail = line[1];\n",
    "            sendersWords[sender].append(splitEmail)\n",
    "            line = next(reader, None)\n",
    "        # starting working with keras to create the model for each sender\n",
    "        for k in range(0, len(senderlist)):\n",
    "            senderEmailsArr = sendersWords[senderlist[k]];\n",
    "            model = Sequential()\n",
    "            n_hidden = 256\n",
    "            n_fac = 42\n",
    "            input_list = []\n",
    "            output_list = []\n",
    "            # list for each sentence with first word and number of words\n",
    "            first_word_list = []\n",
    "            allEmails = \"\"\n",
    "            for email in senderEmailsArr:\n",
    "                allEmails = allEmails + email + \"--\"\n",
    "                emailSplit = email.split(\" \")\n",
    "                firstWord = \"\"\n",
    "                for i in range(0, len(emailSplit)):\n",
    "                    if (emailSplit[i] != ''):\n",
    "                        firstWord = emailSplit[i]\n",
    "                        break\n",
    "                first_word_list.append((firstWord, len(emailSplit)))\n",
    "            allEmails = allEmails[:-2]\n",
    "            vocabulary_size = 800\n",
    "            unknown_token = \"UNKNOWNTOKEN\"\n",
    "            sentence_start_token = \"SENTENCESTART\"\n",
    "            sentence_end_token = \"SENTENCEEND\"\n",
    "            line_break = \"NEWLINE\"\n",
    "            separator = \"SEPARATOR\"\n",
    "            emailText = allEmails.replace('\\n', ' ' + line_break + ' ')\n",
    "            emailText = emailText.replace('--', ' ' + separator + ' ')\n",
    "            emailText = emailText.replace('.', ' ' + sentence_end_token + ' ' + sentence_start_token + ' ')\n",
    "            emailText = re.sub(r'\\d+', '', emailText)\n",
    "            emailTextSeq = text_to_word_sequence(emailText, lower=True, split=\" \")  # using only 10000 first words\n",
    "\n",
    "            token = Tokenizer(nb_words=vocabulary_size, char_level=False)\n",
    "            token.fit_on_texts(emailTextSeq)\n",
    "            text_mtx = token.texts_to_matrix(emailTextSeq, mode='binary')\n",
    "            input_ = text_mtx[:-1]\n",
    "            output_ = text_mtx[1:]\n",
    "            print('input shape is: ' + str(input_.shape) + ' output shape is: ' + str(output_.shape))\n",
    "            # training the model\n",
    "            model=loadModelFromFile(senderlist[k])\n",
    "            #evaluate the model\n",
    "            scores = model.evaluate(input_, output_, verbose=0)\n",
    "            print(\"sender \"+senderlist[k]+ \" accuracy is: \")\n",
    "            print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "#main\n",
    "senderlist=['dean@bgu.ac.il','peler@exchange.bgu.ac.il','bitahon@bgu.ac.il','career@bgu.ac.il','shanigu@bgu.ac.il']\n",
    "#createModels('filteredBySendersTranslated',senderlist)\n",
    "#the accuracy of the model checked with cross validation of 0.2 and saved on the model, after trying to change the features values, the values that\n",
    "#are we chose to use, are fitted to out data and our resources (cpu, memory, etc...)\n",
    "evaluateTheModel('filteredBySendersTranslated',senderlist)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes:\n",
    "##### Input / output examples:\n",
    "input: (real email)\n",
    "We would like to inform you that as of today, Sunday, the Chumusia Delali is closed until the end of the tender process.\"\n",
    "\n",
    "output: (email of the same sender with the model)\n",
    "additional his fund for ac of months does there students' married il the david french french the of nis are underwood who income sorekay economic students \n",
    "\n",
    "\n",
    "more raw emails on filteredBySendersTranslated.csv file.\n",
    "and sender outputs are on <name>_pred.csv file\n",
    "\n",
    "Link to download (too big for github):\n",
    "https://mega.nz/#!P1E32KZQ!k5ecRYt5ih4djz8pYvDXpC5dzkfbCkIpXD8M6N0Ca3s\n",
    "\n",
    "this link contains also the the model files\n",
    "instead build the model, there is a function that load it from a file\n",
    "\n",
    "Previous parts repositories:\n",
    "https://github.com/BenEfrati/ex1/tree/master/email-nlp\n",
    "https://github.com/BenEfrati/ex1/tree/master/email-nlp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
